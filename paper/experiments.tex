\section{Experiments}
\label{sec:experiments}

We evaluate CGVD across three VLA architectures on tabletop manipulation tasks with controlled distractor injection. Our experiments address three questions: (1)~Does CGVD improve clutter robustness across different VLA architectures and tasks? (2)~How does performance scale with distractor density? (3)~How do individual CGVD components contribute?

\subsection{Experimental Setup}

\textbf{Environment.} We evaluate in SimplerEnv~\cite{simplerenv}, a high-fidelity simulation benchmark with demonstrated real-to-sim correlation for VLA evaluation. All experiments use the WidowX robotic arm with a single fixed third-person camera, matching the Bridge dataset training setup.

\textbf{Tasks.} We evaluate on two pick-and-place tasks drawn from the Bridge training set:
\begin{itemize}
    \item \emph{Spoon on towel}: pick up a spoon and place it on a towel.
    \item \emph{Carrot on plate}: pick up a carrot and place it on a plate (24 episode variations).
\end{itemize}
Both tasks use objects present in the Bridge training set (\texttt{bridge\_spoon}, \texttt{bridge\_carrot}). To probe the effect of target familiarity (\S\ref{sec:familiarity}), we additionally evaluate \emph{target-swap} variants that substitute a novel object with identical task kinematics: \emph{cup on towel} and \emph{banana on plate}.

\textbf{VLA models.} We evaluate three architectures, all fine-tuned on the Bridge dataset:
\begin{itemize}
    \item \textbf{Pi0}~\cite{pi0}: Flow matching with mixture-of-experts (PaliGemma-3B backbone, 2.6B parameters).
    \item \textbf{GR00T}~\cite{gr00t}: NVIDIA's generalist robot model (N1.6-3B), fine-tuned on Bridge via LoRA.
    \item \textbf{OpenVLA}~\cite{openvla}: Open-source 7B-parameter VLA built on a pre-trained VLM backbone, fine-tuned on Bridge.
\end{itemize}

\textbf{Distractor categories.} We evaluate two distractor categories to isolate the effect of semantic similarity:
\begin{itemize}
    \item \emph{Semantic confusors}: Objects sharing functional affordances with the target. For plate tasks: other vegetables (corn, cucumber, eggplant) or fruits (apple, orange, lemon). For towel tasks: other utensils (forks, knives, spatulas) or containers.
    \item \emph{Random clutter}: Arbitrary objects with no semantic or visual similarity to the target. Serves as a control condition.
\end{itemize}
Distractors are drawn from RoboCasa~\cite{robocasa} and the YCB dataset~\cite{ycb}, and placed on the workspace using physics-aware grid-based placement with collision avoidance.

\textbf{Protocol.} For each condition, we run 20 episodes per seed with matched random seeds between baseline and CGVD, ensuring identical distractor placements and episode initializations. We vary distractor count from 0 to 10 in increments of 2. We report success rate (SR) as the primary metric. Each condition is repeated across 5 seeds and we report mean $\pm$ standard deviation.

\subsection{Main Results}
\label{sec:main_results}

Figure~\ref{fig:scaling} shows success rates as distractor count increases from 0 to 10 for both semantic confusors and random clutter. We observe several consistent trends. First, baseline performance (dashed lines) degrades monotonically with distractor count across all models and tasks, confirming that clutter systematically impairs VLA control. Second, CGVD (solid lines) maintains a higher success rate floor, with the gap widening as clutter increases---demonstrating that its benefit grows with task difficulty. At $n\!=\!10$ semantic confusors, CGVD improves SR by +18.5 (Pi0), +16.0 (GR00T), and +16.5 (OpenVLA) on spoon-on-towel, with similar gains on carrot-on-plate. Third, at 0 distractors, baseline and CGVD perform identically, verifying the monotonicity guarantee: CGVD never harms performance when there is nothing to remove.

Comparing the two distractor categories (left vs.\ right columns), semantic confusors cause steeper baseline degradation than random clutter, consistent with the intuition that functionally similar objects are more disruptive to VLA target identification. CGVD's benefit is correspondingly larger for semantic confusors, validating the concept-gated design.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/distractor_scaling.pdf}
  \caption{Success rate vs.\ number of distractors. \textbf{Left:} semantic confusors. \textbf{Right:} random clutter. \textbf{Top:} spoon on towel. \textbf{Bottom:} carrot on plate. Dashed lines: baseline; solid lines: +CGVD. Colors denote models. Shaded regions show $\pm$1 std across seeds.}
  \label{fig:scaling}
\end{figure*}

% Generate with: python scripts/plot_distractor_scaling.py --results_dir logs/clutter_eval --output figures/distractor_scaling.pdf

\subsection{Qualitative Analysis}
\label{sec:qualitative}

To visualize what CGVD removes, Figure~\ref{fig:qualitative} shows representative frames from spoon-on-towel episodes at $n\!=\!6$ semantic confusors. The baseline observation (left) contains multiple utensils that compete for the VLA's attention; the CGVD-processed observation (right) retains only the target spoon and anchor towel against a cleanly inpainted background.

% TODO: Generate qualitative figure from attention capture implementation
% (accumulated attention heatmaps overlaid on baseline vs CGVD observations).

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/qualitative.pdf}
  \caption{Qualitative comparison on \emph{spoon on towel} with 6 semantic confusors. \textbf{Left:} raw observation with distractors visible. \textbf{Right:} CGVD-processed observation showing inpainted background with only the target and anchor retained. Bottom row overlays accumulated attention heatmaps from the VLA's vision encoder, illustrating how distractor removal concentrates attention on task-relevant regions.}
  \label{fig:qualitative}
\end{figure*}

\subsection{Effect of Target Familiarity}
\label{sec:familiarity}

The results above use objects present in the Bridge training set. How does CGVD's benefit change when the target object is novel? To answer this, we swap the target object while keeping everything else fixed: \emph{spoon}$\to$\emph{cup} (towel task) and \emph{carrot}$\to$\emph{banana} (plate task). These target-swap variants share identical kinematics, episode variations, and anchor objects, isolating the effect of target familiarity.

\begin{table}[t]
\centering
\caption{Effect of target familiarity (Pi0). ID = Bridge training object; OOD = novel object with identical task kinematics. Clean = no distractors; $n\!=\!10$ = 10 semantic confusors.}
\label{tab:familiarity}
\begin{tabular}{llcccc}
\toprule
Task & Status & Clean SR & $n\!=\!10$ Base & $n\!=\!10$ +CGVD & $\Delta$ \\
\midrule
Spoon on towel    & ID  & -- & 48.5 & 67.0  & +18.5 \\
Cup on towel      & OOD & -- & --   & --    & --    \\
\midrule
Carrot on plate   & ID  & 57.0 $\pm$ 6.4 & -- & -- & -- \\
Banana on plate   & OOD & --              & -- & -- & -- \\
\bottomrule
\end{tabular}
% TODO: Fill in all cells.
\end{table}

Table~\ref{tab:familiarity} reveals a clear pattern: CGVD's benefit is largest for OOD targets. We hypothesize this reflects \emph{task-specific memorization}: VLAs fine-tuned on Bridge learn to recognize the specific training object mesh (e.g., the Bridge carrot) rather than acquiring a generalizable pick-and-place primitive. For ID targets, the model's rigid visual template is relatively resistant to distractor confusion; for OOD targets, the model must rely on semantic features for target identification, making it far more susceptible to distractor interference---and far more amenable to CGVD's intervention.

This has practical implications: in real-world deployment, robots encounter novel objects not present in their training data. CGVD's utility is greatest precisely in this regime, where semantic-level clutter most disrupts target identification.

\subsection{Ablation Studies}

We ablate key CGVD components on spoon-on-towel with 10 semantic confusors (Pi0).

\begin{table}[t]
\centering
\caption{Ablation study on \emph{spoon on towel} (Pi0, 10 semantic distractors). Each row removes one component from the full CGVD pipeline.}
\label{tab:ablation}
\begin{tabular}{lcc}
\toprule
Configuration & SR (\%) & h-SR (\%) \\
\midrule
Baseline (no intervention)             & 48.5 & 39.5 \\
\midrule
CGVD (full pipeline)                   & \textbf{67.0} & \textbf{63.0} \\
\quad $-$ safe-set subtraction         & -- & -- \\
\quad $-$ inpainting (mean-color fill) & -- & -- \\
\bottomrule
\end{tabular}
% TODO: Fill in ablation numbers.
% Run with: --cgvd_disable_safeset, --cgvd_no_inpaint
\end{table}

\textbf{Safe-set subtraction} is the most critical component: without it, the target object can be accidentally inpainted away when SAM confuses it with a semantically similar distractor (e.g., spatula detected as ``spoon'').

\textbf{Inpainting vs.\ mean-color fill} tests whether realistic scene completion matters, or whether simply removing distractor pixels suffices. LaMa inpainting produces more in-distribution observations for VLAs trained on clean scenes, leading to better performance than na\"ive fill.

\subsection{Latency Analysis}

Table~\ref{tab:latency} reports the computational overhead of CGVD.

\begin{table}[t]
\centering
\caption{Per-frame latency breakdown (ms). Warmup frames (first 5) require full segmentation; post-warmup only requires robot tracking, with all other components cached.}
\label{tab:latency}
\begin{tabular}{lcc}
\toprule
Component & Warmup & Post-warmup \\
\midrule
SAM (distractors)     & --  & 0 (cached) \\
SAM (target + anchor) & --  & 0 (cached) \\
SAM (robot tracking)  & --  & -- \\
LaMa inpainting       & --  & 0 (cached) \\
Compositing           & --  & -- \\
\midrule
\textbf{Total CGVD}   & --  & -- \\
VLA inference          & --  & -- \\
\textbf{Total per-frame} & -- & -- \\
\bottomrule
\end{tabular}
% TODO: Fill in from get_timing_stats().
\end{table}

Post-warmup, CGVD's overhead is dominated by a single SAM query for robot tracking. The inpainted background and object masks are cached and reused, exploiting the fixed-camera, stationary-distractor assumption. This reduces steady-state overhead to a fraction of the VLA's own inference time, in contrast to BYOVLA~\cite{byovla} which requires $K$ additional VLA forward passes per region ($\sim$2s/action) and OBEYED-VLA~\cite{obeyed} which runs a full 8B-parameter VLM ($\sim$0.88s/inference).
