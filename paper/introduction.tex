\section{Introduction}
\label{sec:introduction}

The pursuit of general-purpose robotic manipulation has been fundamentally accelerated by the advent of Vision-Language-Action (VLA) models~\cite{openvla, rt2, octo, gr00t, pi0}. By grounding large language models into robotic control policies, these architectures have demonstrated remarkable zero-shot generalization capabilities, enabling robots to follow open-vocabulary instructions like ``put the eggplant in the pot'' without task-specific training. These models promise a future where robots can operate in unstructured, human-centric environments.

However, a significant gap remains between the semantic reasoning capabilities of these models and their geometric precision under real-world deployment conditions. While VLAs excel in curated, clutter-free environments, their performance degrades precipitously in the presence of visual clutter~\cite{distracted_robot, eva_vla}. We term this phenomenon the \emph{Precision-Reasoning Gap}: the model successfully identifies the target object conceptually, yet attention corruption from surrounding distractors dilutes the latent representation used for spatial planning~\cite{obeyed}. This feature dilution manifests as high-variance trajectories, hesitation near distractors, and ultimately manipulation failure.

Critically, this degradation is not uniform across distractor types. We observe that failure concentrates around distractors sharing visual or semantic properties with the target~\cite{distracted_robot, vla_risk}. While VLAs may exhibit resilience to arbitrary clutter via large-scale pre-training, they remain brittle to \emph{semantically confusable} objects---a fork scattered near a target spoon triggers conflicting visual tokens within the same affordance category, causing the policy to attend to, or even grasp, the wrong object.

Existing approaches to mitigate clutter-induced failure fall into three primary categories. \emph{Adaptation methods}, such as OBEYED-VLA~\cite{obeyed}, fine-tune attention adapters to focus on targets. While effective, this requires expensive, architecture-specific retraining and limits generalization to the fine-tuning distribution. \emph{Inference-time intervention methods}, such as BYOVLA~\cite{byovla}, use a VLM to identify distractors and a sensitivity probe to determine which to remove. However, this approach relies on external API calls (GPT-4o), requires multiple VLA forward passes per region for its probe, and provides only probabilistic protection---the target can still be modified if both the VLM and the sensitivity threshold fail to flag it. ARRO~\cite{arro} takes a more aggressive approach, replacing the entire background with a virtual grid and preserving only tracked objects, but this destroys scene context and provides no fallback when the tracker loses the target. \emph{Training-time augmentation methods}~\cite{rosie, genaug, nice} generate diverse cluttered training data via generative models, improving robustness at the cost of retraining and without guarantees at deployment.

To bridge this gap without the cost of retraining or the fragility of existing inference-time approaches, we propose \emph{Concept-Gated Visual Distillation} (CGVD): a model-agnostic inference framework that leverages modern vision foundation models~\cite{sam2} to selectively restructure visual observations before they reach the VLA policy. CGVD parses the task instruction to identify target and anchor objects, segments both distractors and task-relevant entities independently, and uses set-theoretic subtraction to produce a distractor mask from which the target is \emph{architecturally excluded}. Distractors are then replaced via high-quality inpainting~\cite{lama}, preserving the full spatial context of the scene.

Our contributions are as follows:
\begin{itemize}
    \item \textbf{Concept-Gated Visual Distillation (CGVD):} We introduce a training-free, model-agnostic inference framework that selectively removes distractors from VLA observations via language-grounded segmentation and inpainting, while preserving scene context. Unlike prior inference-time methods~\cite{byovla, arro}, CGVD requires zero VLA forward passes and no external API dependencies.

    \item \textbf{Safe-Set Protection with Formal Guarantees:} We propose a dual-mask architecture where task-relevant objects (target, anchor, robot) are independently segmented and subtracted from the distractor mask, providing an architectural guarantee that the target object is never modified. A cross-validation mechanism resolves ambiguous detections between visually confusable objects, and an automatic fallback disables CGVD when the target is undetectable, ensuring $\text{SR}_{\text{CGVD}} \geq \text{SR}_{\text{baseline}}$.

    \item \textbf{Systematic Clutter Robustness Evaluation:} We evaluate CGVD across multiple VLA architectures on SimplerEnv manipulation tasks with controlled distractor injection, demonstrating significant improvements in clutter tolerance. % TODO: Add final numbers and real-hardware results.
\end{itemize}
